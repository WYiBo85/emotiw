{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.0-la-tj-ak-kc-vl-ensemble_baseline_predictions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevincong95/cs231n-emotiw/blob/master/notebooks/2.0-la-tj-ak-ensemble_baseline_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rWTS99OI8l-",
        "colab_type": "text"
      },
      "source": [
        "## Video Sentiment Analysis in the Wild\n",
        "### Ensembling Notebook | CS231n\n",
        "\n",
        "This notebook preprocesses input videos to extract faces, frames, poses, and audio before running pre-trained models for each modality to predict group sentiment (positive, negative, or neutral). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58avLN7UlDDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86a44408-ae87-4b76-f724-7842066f45e2"
      },
      "source": [
        "# Clone the code base\n",
        "!git clone 'https://github.com/kevincong95/cs231n-emotiw.git'\n",
        "\n",
        "# Switch to TF 1.x and navigate to the directory\n",
        "%tensorflow_version 1.x\n",
        "!pwd\n",
        "import os\n",
        "os.chdir('cs231n-emotiw')\n",
        "!pwd\n",
        "\n",
        "# Install required packages \n",
        "!pip install -r 'requirements-predictions.txt'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cs231n-emotiw'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 754 (delta 42), reused 27 (delta 16), pack-reused 683\u001b[K\n",
            "Receiving objects: 100% (754/754), 175.22 MiB | 38.52 MiB/s, done.\n",
            "Resolving deltas: 100% (450/450), done.\n",
            "TensorFlow 1.x selected.\n",
            "/content\n",
            "/content/cs231n-emotiw\n",
            "Requirement already satisfied: tensorflow in /tensorflow-1.15.2/python3.6 (from -r requirements-predictions.txt (line 1)) (1.15.2)\n",
            "Requirement already satisfied: opencv-python>=4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 2)) (4.1.2.30)\n",
            "Collecting pydub==0.24.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/f9/2cd255898c11179a57415937d601ab1e8a14a7c6a8331ff9c365e97e41f6/pydub-0.24.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 4)) (3.2.1)\n",
            "Collecting moviepy>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/54/01a8c4e35c75ca9724d19a7e4de9dc23f0ceb8769102c7de056113af61c3/moviepy-1.0.3.tar.gz (388kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 6)) (2.1.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 7)) (1.0.3)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 8)) (0.7)\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 11)) (1.18.4)\n",
            "Collecting openl3\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/88/8536723b81c47ada614d008a75e71923932f22e35ab89d4bf5fe441b58fc/openl3-0.3.1.tar.gz\n",
            "Collecting SoundFile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 15.3MB/s \n",
            "\u001b[?25hCollecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 16)) (19.18.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.29.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow->-r requirements-predictions.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow->-r requirements-predictions.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2.23.0)\n",
            "Collecting proglog<=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/ab/4cb19b578e1364c0b2d6efd6521a8b4b4e5c4ae6528041d31a2a951dd991/proglog-0.1.9.tar.gz\n",
            "Collecting imageio<3.0,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/2b/9dd19644f871b10f7e32eb2dbd6b45149c350b4d5f2893e091b882e03ab7/imageio-2.8.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 23.9MB/s \n",
            "\u001b[?25hCollecting imageio_ffmpeg>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/c8/04c6b4a001b8ae7326fb83d6665af1ee58d6cc1acb421f8ea40d2678fe3c/imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9MB 115kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements-predictions.txt (line 7)) (2018.9)\n",
            "Collecting keras<2.3.0,>=2.0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (1.4.1)\n",
            "Collecting kapre==0.1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/2e/f540d1d1f05c764686163fdb5bb1e5c703f1528076d2829bfc3900683f06/kapre-0.1.4-py3-none-any.whl\n",
            "Collecting PySoundFile>=0.9.0.post1\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: resampy<0.3.0,>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (0.2.2)\n",
            "Requirement already satisfied: h5py<3.0.0,>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (2.10.0)\n",
            "Collecting scikit-image<0.15.0,>=0.14.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/78/cfb15cdb3f63eea16946a42d0dbf7ef17be79d30858aa8efd5f6757bd106/scikit_image-0.14.5-cp36-cp36m-manylinux1_x86_64.whl (25.4MB)\n",
            "\u001b[K     |████████████████████████████████| 25.4MB 122kB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile->-r requirements-predictions.txt (line 13)) (1.14.0)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 30kB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition->-r requirements-predictions.txt (line 15)) (7.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition->-r requirements-predictions.txt (line 15)) (7.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->-r requirements-predictions.txt (line 1)) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2020.4.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<2.3.0,>=2.0.9->openl3->-r requirements-predictions.txt (line 12)) (3.13)\n",
            "Requirement already satisfied: librosa>=0.5 in /usr/local/lib/python3.6/dist-packages (from kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.16.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy<0.3.0,>=0.2.1->openl3->-r requirements-predictions.txt (line 12)) (0.48.0)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15.0,>=0.14.3->openl3->-r requirements-predictions.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15.0,>=0.14.3->openl3->-r requirements-predictions.txt (line 12)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15.0,>=0.14.3->openl3->-r requirements-predictions.txt (line 12)) (2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile->-r requirements-predictions.txt (line 13)) (2.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5->kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5->kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.15.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy<0.3.0,>=0.2.1->openl3->-r requirements-predictions.txt (line 12)) (0.31.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (3.1.0)\n",
            "Building wheels for collected packages: moviepy, openl3, imgaug, gast, proglog, face-recognition-models\n",
            "  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for moviepy: filename=moviepy-1.0.3-cp36-none-any.whl size=110728 sha256=e033b553a1ab145e85033de0edfc035685703d793ef5a4eb2400d84c2f461fd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/fe/1c/f4e6dca9e828d4b979c04e461d7fcc5b8e7bd35f947e665b65\n",
            "  Building wheel for openl3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openl3: filename=openl3-0.3.1-py2.py3-none-any.whl size=249323247 sha256=cc7d4c9114cb73ebd29b72245d964d27d4c352406c437289a790dd66ac11a442\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/63/c9/35868f3dd3b466909e73178db8566430da2d093cc055b932b1\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=a3cef3c74c1e3db07edfa23120676734f05806c34388601dcc26c20efb74a08b\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=8ccfad44a984b5cfd296b1105ab92f2921f754cba70e319cdb86028536bc3a71\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for proglog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proglog: filename=proglog-0.1.9-cp36-none-any.whl size=6149 sha256=f11937d4f478805388d5bce95f8258560865dede59f87956447aed50fd972e17\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/56/60/1d0306a8d90b188af393c1812ddb502a8821b70917f82dcc00\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566172 sha256=3ba7ae7f986d2e08775358d79a08a4893be2d0823ce8d64a0c758c1b6a95189b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built moviepy openl3 imgaug gast proglog face-recognition-models\n",
            "Installing collected packages: pydub, proglog, imageio, imageio-ffmpeg, moviepy, argparse, keras, kapre, PySoundFile, scikit-image, openl3, SoundFile, imgaug, face-recognition-models, face-recognition, gast\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Found existing installation: moviepy 0.2.3.5\n",
            "    Uninstalling moviepy-0.2.3.5:\n",
            "      Successfully uninstalled moviepy-0.2.3.5\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "  Found existing installation: kapre 0.1.3.1\n",
            "    Uninstalling kapre-0.1.3.1:\n",
            "      Successfully uninstalled kapre-0.1.3.1\n",
            "  Found existing installation: scikit-image 0.16.2\n",
            "    Uninstalling scikit-image-0.16.2:\n",
            "      Successfully uninstalled scikit-image-0.16.2\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed PySoundFile-0.9.0.post1 SoundFile-0.10.3.post1 argparse-1.4.0 face-recognition-1.3.0 face-recognition-models-0.3.0 gast-0.2.2 imageio-2.8.0 imageio-ffmpeg-0.4.2 imgaug-0.2.6 kapre-0.1.4 keras-2.2.5 moviepy-1.0.3 openl3-0.3.1 proglog-0.1.9 pydub-0.24.0 scikit-image-0.14.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14TqM3dYQ2wk",
        "colab_type": "text"
      },
      "source": [
        "#### Pose Pre-Requisites\n",
        "Pose extraction uses the [CMU OpenPose library](https://github.com/CMU-Perceptual-Computing-Lab/openpose) to extract body keypoints. We have pre-compiled this library for use in Colab but some system files still need to be installed. \n",
        "\n",
        "#### Retrieve the files\n",
        "\n",
        "The code block below demonstrates how to retrieve the files from GCS. However, feel free to skip this step if the files are already on the local disk or you have Google Drive mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpkaHvUGf9Hi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "65b0f2dc-8d0a-4a8a-e569-4dc95c494678"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data\t   notebooks   requirements-predictions.txt\t\t   src\n",
            " LICENSE   README.md   requirements.txt\n",
            " models    reports    'Screen Shot 2020-05-26 at 8.53.32 PM.png'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qsmv0bdQJQp",
        "colab_type": "code",
        "outputId": "e99bc9a9-a5d0-4f39-9a6a-44ca0aa451ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/openpose/openpose.tar.gz\n",
        "!tar -xzf openpose.tar.gz\n",
        "\n",
        "# The pre-built OpenPose library contains shared library files that need to be manually linked\n",
        "import os\n",
        "orig_lib_path = os.environ['LD_LIBRARY_PATH']\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = f\"{orig_lib_path}:{os.getcwd()}/openpose/build/src/openpose/:{os.getcwd()}/openpose/build/caffe/lib/\"\n",
        "\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/train-tiny.zip\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/val-tiny.zip\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/test-tiny.zip\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/Train_labels.txt\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/Val_labels.txt\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libgflags2.2.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libgflags2.2_2.2.1-1_amd64.deb ...\n",
            "Unpacking libgflags2.2 (2.2.1-1) ...\n",
            "Selecting previously unselected package libgflags-dev.\n",
            "Preparing to unpack .../01-libgflags-dev_2.2.1-1_amd64.deb ...\n",
            "Unpacking libgflags-dev (2.2.1-1) ...\n",
            "Selecting previously unselected package libgoogle-glog0v5.\n",
            "Preparing to unpack .../02-libgoogle-glog0v5_0.3.5-1_amd64.deb ...\n",
            "Unpacking libgoogle-glog0v5 (0.3.5-1) ...\n",
            "Selecting previously unselected package libgoogle-glog-dev.\n",
            "Preparing to unpack .../03-libgoogle-glog-dev_0.3.5-1_amd64.deb ...\n",
            "Unpacking libgoogle-glog-dev (0.3.5-1) ...\n",
            "Selecting previously unselected package libhdf5-serial-dev.\n",
            "Preparing to unpack .../04-libhdf5-serial-dev_1.10.0-patch1+docs-4_all.deb ...\n",
            "Unpacking libhdf5-serial-dev (1.10.0-patch1+docs-4) ...\n",
            "Selecting previously unselected package libleveldb1v5:amd64.\n",
            "Preparing to unpack .../05-libleveldb1v5_1.20-2_amd64.deb ...\n",
            "Unpacking libleveldb1v5:amd64 (1.20-2) ...\n",
            "Selecting previously unselected package libleveldb-dev:amd64.\n",
            "Preparing to unpack .../06-libleveldb-dev_1.20-2_amd64.deb ...\n",
            "Unpacking libleveldb-dev:amd64 (1.20-2) ...\n",
            "Selecting previously unselected package liblmdb0:amd64.\n",
            "Preparing to unpack .../07-liblmdb0_0.9.21-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking liblmdb0:amd64 (0.9.21-1ubuntu0.1) ...\n",
            "Selecting previously unselected package liblmdb-dev:amd64.\n",
            "Preparing to unpack .../08-liblmdb-dev_0.9.21-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking liblmdb-dev:amd64 (0.9.21-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libprotobuf-lite10:amd64.\n",
            "Preparing to unpack .../09-libprotobuf-lite10_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package lmdb-doc.\n",
            "Preparing to unpack .../10-lmdb-doc_0.9.21-1ubuntu0.1_all.deb ...\n",
            "Unpacking lmdb-doc (0.9.21-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\n",
            "Preparing to unpack .../11-libprotobuf-dev_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libsnappy-dev:amd64.\n",
            "Preparing to unpack .../12-libsnappy-dev_1.1.7-1_amd64.deb ...\n",
            "Unpacking libsnappy-dev:amd64 (1.1.7-1) ...\n",
            "Selecting previously unselected package libviennacl-dev.\n",
            "Preparing to unpack .../13-libviennacl-dev_1.7.1+dfsg1-2ubuntu1_all.deb ...\n",
            "Unpacking libviennacl-dev (1.7.1+dfsg1-2ubuntu1) ...\n",
            "Selecting previously unselected package opencl-clhpp-headers.\n",
            "Preparing to unpack .../14-opencl-clhpp-headers_2.0.10+git12-g5dd8bb9-1_all.deb ...\n",
            "Unpacking opencl-clhpp-headers (2.0.10+git12-g5dd8bb9-1) ...\n",
            "Selecting previously unselected package opencl-headers.\n",
            "Preparing to unpack .../15-opencl-headers_2.2~2018.02.21-gb5c3680-1_all.deb ...\n",
            "Unpacking opencl-headers (2.2~2018.02.21-gb5c3680-1) ...\n",
            "Setting up libviennacl-dev (1.7.1+dfsg1-2ubuntu1) ...\n",
            "Setting up libgflags2.2 (2.2.1-1) ...\n",
            "Setting up libgflags-dev (2.2.1-1) ...\n",
            "Setting up liblmdb0:amd64 (0.9.21-1ubuntu0.1) ...\n",
            "Setting up opencl-clhpp-headers (2.0.10+git12-g5dd8bb9-1) ...\n",
            "Setting up libleveldb1v5:amd64 (1.20-2) ...\n",
            "Setting up libhdf5-serial-dev (1.10.0-patch1+docs-4) ...\n",
            "Setting up libsnappy-dev:amd64 (1.1.7-1) ...\n",
            "Setting up libgoogle-glog0v5 (0.3.5-1) ...\n",
            "Setting up liblmdb-dev:amd64 (0.9.21-1ubuntu0.1) ...\n",
            "Setting up lmdb-doc (0.9.21-1ubuntu0.1) ...\n",
            "Setting up libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up opencl-headers (2.2~2018.02.21-gb5c3680-1) ...\n",
            "Setting up libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libleveldb-dev:amd64 (1.20-2) ...\n",
            "Setting up libgoogle-glog-dev (0.3.5-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "--2020-06-02 05:27:56--  https://storage.googleapis.com/cs231n-emotiw/openpose/openpose.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 598708936 (571M) [application/x-tar]\n",
            "Saving to: ‘openpose.tar.gz’\n",
            "\n",
            "openpose.tar.gz     100%[===================>] 570.97M   125MB/s    in 4.9s    \n",
            "\n",
            "2020-06-02 05:28:01 (117 MB/s) - ‘openpose.tar.gz’ saved [598708936/598708936]\n",
            "\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmTc8EMiFFzt",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess Files\n",
        "\n",
        "Here, we will instantiate each of the preprocessors and process all of the input video files.\n",
        "\n",
        "NOTE: Change the input parameters as needed.\n",
        "\n",
        "WARNING: This may take several hours to complete, depending on the number of files.\n",
        "\n",
        "In general, pre-processing will extract the following:\n",
        "- Video frames\n",
        "- Pose keypoints\n",
        "- Faces from each video frame\n",
        "- Audio waveform and audio features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36LTqWyJFHGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from src.preprocessors.preprocess_all_modes import preprocess\n",
        "from src.preprocessors.pose_preprocessor import PosePreprocessor\n",
        "\n",
        "print(\"Starting to preprocess train data\")\n",
        "preprocess(video_folder=\"train-tiny.zip\", label_file=\"Train_labels.txt\", local_base_path=\"train-tiny\")\n",
        "\n",
        "print(\"Starting to preprocess val data\")\n",
        "preprocess(video_folder=\"val-tiny.zip\", label_file=\"Val_labels.txt\", local_base_path=\"val-tiny\")\n",
        "\n",
        "print(\"Starting to preprocess test data\")\n",
        "preprocess(video_folder=\"test-tiny.zip\", local_base_path=\"test-tiny\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc0OOW4vVI7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the openpose folder as it is no longer required\n",
        "!rm -rf openpose/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f141DYtugV0T",
        "colab_type": "code",
        "outputId": "31344153-6c8b-482e-c9bd-e719e0563303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data\t\t\t\t\t     train-tiny-audio_tmp\n",
            " LICENSE\t\t\t\t     train-tiny-audio.zip\n",
            " models\t\t\t\t\t     train-tiny-faces\n",
            " notebooks\t\t\t\t     train-tiny-faces_tmp\n",
            " openpose.tar.gz\t\t\t     train-tiny-faces.zip\n",
            " README.md\t\t\t\t     train-tiny-frames\n",
            " reports\t\t\t\t     train-tiny-frames_tmp\n",
            " requirements-predictions.txt\t\t     train-tiny-frames.zip\n",
            " requirements.txt\t\t\t     train-tiny-pose\n",
            "'Screen Shot 2020-05-26 at 8.53.32 PM.png'   train-tiny-pose.zip\n",
            " src\t\t\t\t\t     train-tiny.zip\n",
            " test-tiny-audio\t\t\t     Val_labels.txt\n",
            " test-tiny-audio_tmp\t\t\t     val-tiny-audio\n",
            " test-tiny-audio.zip\t\t\t     val-tiny-audio_tmp\n",
            " test-tiny-faces\t\t\t     val-tiny-audio.zip\n",
            " test-tiny-faces_tmp\t\t\t     val-tiny-faces\n",
            " test-tiny-faces.zip\t\t\t     val-tiny-faces_tmp\n",
            " test-tiny-frames\t\t\t     val-tiny-faces.zip\n",
            " test-tiny-frames_tmp\t\t\t     val-tiny-frames\n",
            " test-tiny-frames.zip\t\t\t     val-tiny-frames_tmp\n",
            " test-tiny-pose\t\t\t\t     val-tiny-frames.zip\n",
            " test-tiny-pose.zip\t\t\t     val-tiny-pose\n",
            " test-tiny.zip\t\t\t\t     val-tiny-pose.zip\n",
            " Train_labels.txt\t\t\t     val-tiny.zip\n",
            " train-tiny-audio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1SA_BTamwmW",
        "colab_type": "code",
        "outputId": "feacc58e-11b7-4dd2-ab7b-a6a6f1544c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp train-tiny-*.zip \"/content/drive/My Drive/cs231n-project/datasets/emotiw\"\n",
        "!cp val-tiny-*.zip \"/content/drive/My Drive/cs231n-project/datasets/emotiw\"\n",
        "!cp test-tiny-*.zip \"/content/drive/My Drive/cs231n-project/datasets/emotiw\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEaDQAsS8XQ",
        "colab_type": "text"
      },
      "source": [
        "### Run Classifiers\n",
        "\n",
        "**IMPORTANT**: You must restart the runtime at this point to use TF 2.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0_gfpBjXP6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPXBx950Xd8z",
        "colab_type": "code",
        "outputId": "980509cf-4edb-475d-dc53-da43e9ca7891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuMdFr8GT8uW",
        "colab_type": "code",
        "outputId": "f4f8a775-0610-4dbc-b0ea-999e4c2d39ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pwd\n",
        "import os\n",
        "os.chdir('cs231n-emotiw')\n",
        "!pwd\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/cs231n-emotiw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "338bzmlETsvo",
        "colab_type": "code",
        "outputId": "5683dff6-3d99-4b1b-8b75-09ac8b13c59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 42 (delta 31), reused 22 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (42/42), done.\n",
            "From https://github.com/kevincong95/cs231n-emotiw\n",
            "   6256967..8e9c7e4  master     -> origin/master\n",
            " * [new branch]      fer        -> origin/fer\n",
            "Updating 6256967..8e9c7e4\n",
            "Fast-forward\n",
            " notebooks/1.0-la-audio-error-analysis.ipynb |  879 \u001b[32m++++++++++++++++++\u001b[m\n",
            " notebooks/scene/1.0-tj-resnet-lstm.ipynb    | 1271 \u001b[32m++++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " src/generators/frame_generator.py           |    8 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " src/generators/pose_generator.py            |    8 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 4 files changed, 2154 insertions(+), 12 deletions(-)\n",
            " create mode 100644 notebooks/1.0-la-audio-error-analysis.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOIbnIoH_AHV",
        "colab_type": "code",
        "outputId": "ee2ffa1d-2974-4c7d-b11e-a744c50fcd7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from src.classifiers.audio_classifier import AudioClassifier\n",
        "from src.classifiers.frames_classifier import FramesClassifier\n",
        "from src.classifiers.pose_classifier import PoseClassifier\n",
        "from src.classifiers.utils import get_num_samples\n",
        "import numpy as np\n",
        "\n",
        "audio_classifier = AudioClassifier('train-tiny-audio', model_location='https://storage.googleapis.com/cs231n-emotiw/models/openl3-cnn-lstm-tuned-lr.h5', is_test=False)\n",
        "frames_classifier = FramesClassifier('train-tiny-frames', model_location='https://storage.googleapis.com/cs231n-emotiw/models/scene-classifier-resnet-lstm-x3.h5', is_test=False)\n",
        "pose_classifier = PoseClassifier('train-tiny-pose', model_location='https://storage.googleapis.com/cs231n-emotiw/models/pose-classifier-64lstm-0.01reg.h5', is_test=False)\n",
        "#faces_classifier = \n",
        "\n",
        "classifiers = [audio_classifier, frames_classifier, pose_classifier]\n",
        "\n",
        "sample_to_true_label = {}\n",
        "with open(\"Train_labels.txt\") as f:\n",
        "    l = 0\n",
        "    for line in f:\n",
        "        if l == 0:\n",
        "            # Skip headers\n",
        "            l += 1\n",
        "            continue\n",
        "        line_arr = line.split(\" \")\n",
        "        sample_to_true_label[line_arr[0].strip()] = int(line_arr[1].strip())\n",
        "        l += 1\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AudioClassifier created with audio_folder = train-tiny-audio , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/openl3-cnn-lstm-tuned-lr.h5\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "FramesClassifier created with frames_folder = train-tiny-frames , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/scene-classifier-resnet-lstm-x3.h5\n",
            "Downloading data from https://storage.googleapis.com/cs231n-emotiw/models/scene-classifier-resnet-lstm-x3.h5\n",
            "121389056/121384640 [==============================] - 2s 0us/step\n",
            "PoseClassifier created with pose_folder = train-tiny-pose , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/pose-classifier-64lstm-0.01reg.h5\n",
            "Downloading data from https://storage.googleapis.com/cs231n-emotiw/models/pose-classifier-64lstm-0.01reg.h5\n",
            "614400/612904 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkWkBoCgaNWB",
        "colab_type": "code",
        "outputId": "62b8050c-e1be-4da3-c648-9b671948fd38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "audio_classifier.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, 6144)]      0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          786496    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 512)         66048     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 512)         524800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 20)          41840     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 10)                1040      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                352       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 1,425,027\n",
            "Trainable params: 1,422,851\n",
            "Non-trainable params: 2,176\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWrHkaGyb3w_",
        "colab_type": "code",
        "outputId": "b65d43c7-e267-4fae-a21d-a9c22789672c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "frames_classifier.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           [(None, 12, 320, 480 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_7 (TimeDistrib (None, 12, 10, 15, 2 23587712    input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d_18 (ConvLSTM2D)    (None, 12, 10, 15, 1 740920      time_distributed_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d_19 (ConvLSTM2D)    (None, 12, 10, 15, 1 740920      time_distributed_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv_lst_m2d_20 (ConvLSTM2D)    (None, 12, 10, 15, 1 740920      time_distributed_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_18 (Gl (None, 10)           0           conv_lst_m2d_18[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_19 (Gl (None, 10)           0           conv_lst_m2d_19[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d_20 (Gl (None, 10)           0           conv_lst_m2d_20[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 30)           0           global_average_pooling3d_18[0][0]\n",
            "                                                                 global_average_pooling3d_19[0][0]\n",
            "                                                                 global_average_pooling3d_20[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 3)            93          concatenate_5[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 25,810,565\n",
            "Trainable params: 2,222,853\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiN69LZob9G0",
        "colab_type": "code",
        "outputId": "c0435eb5-8c3d-47ec-9805-239de7609d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "pose_classifier.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 12, 27)]          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               47104     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 47,491\n",
            "Trainable params: 47,491\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xwIeRF_x40m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(classifiers, sample_to_true_label=None, mode=\"soft\", complex_fusion=False, weights=[]):\n",
        "    assert mode in [\"soft\" , \"hard\", \"weighted\"]\n",
        "    from collections import Counter\n",
        "\n",
        "    classifier_outputs = []\n",
        "    classifier_samples = []\n",
        "    sample_to_row = {}\n",
        "    num_samples = 0\n",
        "    y_true = []\n",
        "\n",
        "    for c, classifier in enumerate(classifiers):\n",
        "        results, samples = classifier.predict()\n",
        "        classifier_outputs.append(results.tolist())\n",
        "        classifier_samples.append(list(samples))\n",
        "        num_samples = len(list(samples))\n",
        "\n",
        "    print(f\"Number of samples: {num_samples}\")\n",
        "\n",
        "    for i, sample in enumerate(classifier_samples[0]):\n",
        "        sample_to_row[sample] = i\n",
        "        if sample_to_true_label is not None:\n",
        "            y_true.append(sample_to_true_label[sample])\n",
        "\n",
        "    X = np.zeros(shape=(len(classifiers), num_samples, 3))\n",
        "    for c, output in enumerate(classifier_outputs):\n",
        "        samples = classifier_samples[c]\n",
        "        for i, row in enumerate(output):\n",
        "            sample = samples[i]\n",
        "            X[c, sample_to_row[sample], :] += row\n",
        "\n",
        "    print(X[:, 10, :])\n",
        "\n",
        "    if mode == \"weighted\":\n",
        "        for c in range(X.shape[0]):\n",
        "            X[c, :, :] = weights[c] * X[c, :, :]\n",
        "\n",
        "        y_pred = np.sum(X, axis=0)\n",
        "        y_pred = np.argmax(y_pred, axis=1) + 1 # Add 1 because true labels range from 1 to 3\n",
        "        if sample_to_true_label is not None:\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "        print(\"Predicted y-labels:\")\n",
        "        print(y_pred)\n",
        "\n",
        "        if sample_to_true_label is not None:\n",
        "            print(\"True y-labels:\")\n",
        "            print(y_true)\n",
        "\n",
        "            accuracy = (y_pred == y_true).mean()\n",
        "            print(f\"Accuracy: {accuracy}\")\n",
        "            return y_pred , y_true\n",
        "\n",
        "    if mode == \"soft\":\n",
        "        # Take the average of each \n",
        "        y_pred = np.mean(X, axis=0)\n",
        "        y_pred = np.argmax(y_pred, axis=1) + 1 # Add 1 because true labels range from 1 to 3\n",
        "        if sample_to_true_label is not None:\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "        print(\"Predicted y-labels:\")\n",
        "        print(y_pred)\n",
        "\n",
        "        if sample_to_true_label is not None:\n",
        "            print(\"True y-labels:\")\n",
        "            print(y_true)\n",
        "\n",
        "            accuracy = (y_pred == y_true).mean()\n",
        "            print(f\"Accuracy: {accuracy}\")\n",
        "            return y_pred , y_true\n",
        "\n",
        "    if mode == \"hard\":\n",
        "        # Take the majority vote of each\n",
        "\n",
        "        y_pred_tmp = np.zeros((len(classifier_outputs) , len(classifier_outputs[0])))\n",
        "\n",
        "        \n",
        "        for i in range(0 , X.shape[0]):\n",
        "          for j in range(0 , X.shape[1]):\n",
        "            y_pred_tmp[i][j] = np.argmax(X[i][j], axis=0) + 1.0\n",
        "            \n",
        "         \n",
        "        y_pred_tmp = y_pred_tmp.T\n",
        "        y_pred_tmp = np.asarray(y_pred_tmp , dtype='int64')\n",
        "        \n",
        "        y_pred = np.zeros((X.shape[1]))\n",
        "\n",
        "        for i in range(0 , len(y_pred_tmp)):\n",
        "          y_pred_tmp[i] = np.asarray(y_pred_tmp[i] , dtype='int64')\n",
        "          counts = np.bincount(y_pred_tmp[i])\n",
        "          y_pred[i] = np.argmax(counts)\n",
        "\n",
        "        \n",
        "      \n",
        "   # Add 1 because true labels range from 1 to 3\n",
        "        if sample_to_true_label is not None:\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "        print(\"Predicted y-labels:\")\n",
        "        print(y_pred)\n",
        "\n",
        "        if sample_to_true_label is not None:\n",
        "            print(\"True y-labels:\")\n",
        "            print(y_true)\n",
        "\n",
        "            accuracy = (y_pred == y_true).mean()\n",
        "            print(f\"Accuracy: {accuracy}\")\n",
        "            return y_pred , y_true\n",
        "    else:\n",
        "        print(\"Not implemented yet\")\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VGA1fl2pNvE",
        "colab_type": "code",
        "outputId": "1da27c39-cdb7-4bc4-ba30-9c7ab8687f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from src.classifiers.audio_classifier import AudioClassifier\n",
        "from src.classifiers.frames_classifier import FramesClassifier\n",
        "from src.classifiers.pose_classifier import PoseClassifier\n",
        "from src.classifiers.utils import get_num_samples\n",
        "import numpy as np\n",
        "\n",
        "audio_classifier = AudioClassifier('val-tiny-audio', model_location='https://storage.googleapis.com/cs231n-emotiw/models/openl3-cnn-lstm-tuned-lr.h5', is_test=False)\n",
        "frames_classifier = FramesClassifier('val-tiny-frames', model_location='https://storage.googleapis.com/cs231n-emotiw/models/scene-classifier-resnet-lstm-x3.h5', is_test=False)\n",
        "pose_classifier = PoseClassifier('val-tiny-pose', model_location='https://storage.googleapis.com/cs231n-emotiw/models/pose-classifier-64lstm-0.01reg.h5', is_test=False)\n",
        "\n",
        "classifiers = [audio_classifier, frames_classifier, pose_classifier] \n",
        "# classifiers = [audio_classifier] \n",
        "\n",
        "# Audio Classifier = 0.5\n",
        "# Frames classifier = 0.56\n",
        "# Pose classifier = 0.48\n",
        "\n",
        "sample_to_true_label = {}\n",
        "with open(\"Val_labels.txt\") as f:\n",
        "    l = 0\n",
        "    for line in f:\n",
        "        if l == 0:\n",
        "            # Skip headers\n",
        "            l += 1\n",
        "            continue\n",
        "        line_arr = line.split(\" \")\n",
        "        sample_to_true_label[line_arr[0].strip()] = int(line_arr[1].strip())\n",
        "        l += 1\n",
        "print(sample_to_true_label)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AudioClassifier created with audio_folder = val-tiny-audio , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/openl3-cnn-lstm-tuned-lr.h5\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "FramesClassifier created with frames_folder = val-tiny-frames , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/scene-classifier-resnet-lstm-x3.h5\n",
            "PoseClassifier created with pose_folder = val-tiny-pose , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/pose-classifier-64lstm-0.01reg.h5\n",
            "{'1_1': 3, '1_2': 3, '5_1': 1, '5_2': 1, '5_3': 1, '5_4': 1, '5_5': 1, '5_6': 1, '5_7': 1, '5_8': 1, '5_9': 1, '5_10': 1, '5_11': 1, '5_12': 1, '5_13': 1, '5_14': 1, '5_15': 1, '5_16': 1, '5_17': 1, '5_18': 1, '5_19': 1, '11_1': 2, '11_2': 1, '11_3': 1, '11_4': 1, '11_5': 1, '11_6': 1, '11_7': 1, '11_8': 1, '11_9': 1, '11_10': 1, '11_11': 1, '11_12': 1, '11_13': 2, '11_14': 1, '11_15': 1, '11_16': 1, '11_17': 1, '15_2': 2, '15_3': 2, '15_4': 2, '15_5': 1, '15_6': 2, '21_1': 2, '27_2': 1, '27_3': 1, '27_4': 1, '27_5': 2, '27_6': 1, '27_8': 2, '27_9': 2, '27_11': 1, '27_13': 2, '27_14': 2, '27_16': 2, '27_17': 2, '27_18': 1, '27_19': 1, '27_20': 2, '27_21': 2, '31_2': 1, '31_3': 1, '31_4': 1, '31_5': 1, '31_6': 1, '31_7': 1, '31_8': 1, '31_9': 2, '44_1': 1, '44_2': 1, '44_4': 1, '44_6': 1, '44_9': 1, '44_11': 2, '44_12': 1, '44_13': 1, '44_14': 1, '44_15': 2, '44_16': 1, '44_17': 1, '44_18': 1, '44_20': 1, '44_22': 1, '44_23': 1, '44_25': 1, '44_26': 1, '44_28': 1, '44_29': 1, '45_1': 1, '45_2': 1, '45_4': 1, '45_5': 1, '45_6': 1, '45_8': 1, '45_9': 1, '45_10': 1, '53_1': 1, '53_2': 1, '53_3': 1, '53_4': 1, '53_5': 2, '53_6': 1, '53_7': 1, '53_8': 1, '53_9': 1, '53_10': 1, '53_11': 1, '53_12': 1, '53_13': 1, '53_14': 2, '53_15': 3, '53_16': 1, '53_17': 1, '53_18': 1, '53_19': 2, '53_20': 2, '53_21': 1, '53_22': 1, '53_23': 1, '53_24': 1, '53_25': 1, '53_26': 1, '53_27': 1, '53_28': 1, '53_29': 1, '53_30': 2, '53_31': 2, '53_32': 1, '53_33': 2, '53_34': 2, '53_35': 2, '53_36': 2, '53_37': 2, '53_38': 2, '53_39': 2, '53_40': 2, '53_41': 1, '53_42': 1, '53_43': 2, '53_44': 2, '53_45': 1, '53_46': 1, '53_47': 1, '53_48': 1, '53_49': 1, '53_50': 1, '54_1': 2, '54_3': 1, '54_4': 2, '54_5': 2, '54_7': 1, '54_8': 1, '54_9': 1, '54_10': 2, '54_11': 1, '54_12': 1, '54_13': 2, '54_14': 2, '54_15': 2, '54_16': 1, '54_17': 2, '54_18': 1, '54_19': 2, '55_2': 2, '55_3': 2, '55_4': 2, '55_5': 2, '55_7': 2, '55_8': 2, '55_9': 2, '55_10': 2, '55_11': 2, '55_13': 2, '55_14': 2, '55_15': 2, '55_16': 2, '55_17': 2, '55_18': 1, '55_19': 2, '55_20': 2, '55_21': 2, '55_22': 2, '55_23': 2, '55_25': 2, '70_3': 1, '70_4': 1, '70_6': 1, '70_7': 1, '70_8': 1, '70_9': 1, '70_10': 2, '70_11': 1, '70_12': 1, '70_14': 1, '70_15': 2, '70_17': 1, '70_19': 2, '70_20': 2, '70_21': 2, '70_22': 2, '70_23': 2, '70_24': 2, '70_25': 2, '70_26': 1, '70_27': 1, '70_29': 1, '70_30': 2, '77_3': 1, '77_4': 2, '81_1': 2, '81_4': 2, '81_6': 1, '81_7': 2, '81_8': 2, '81_9': 2, '81_13': 2, '81_14': 2, '81_17': 2, '81_18': 2, '81_19': 2, '81_21': 2, '81_23': 2, '81_24': 2, '81_25': 2, '83_1': 2, '86_2': 1, '86_3': 1, '86_4': 1, '86_5': 1, '86_6': 1, '86_7': 1, '86_8': 1, '86_10': 1, '86_11': 1, '86_12': 1, '86_13': 1, '86_14': 1, '86_16': 1, '86_17': 3, '86_18': 3, '86_19': 3, '86_20': 3, '86_21': 3, '86_23': 3, '86_24': 3, '86_26': 1, '86_27': 3, '86_28': 3, '86_29': 3, '86_30': 3, '86_31': 1, '86_32': 3, '86_33': 2, '86_34': 3, '86_35': 1, '86_36': 3, '86_37': 3, '86_38': 3, '86_39': 3, '86_40': 1, '86_41': 1, '86_42': 1, '86_43': 1, '86_44': 1, '86_47': 1, '86_48': 3, '86_49': 3, '86_50': 1, '86_54': 1, '86_55': 1, '86_56': 1, '86_58': 1, '98_3': 3, '99_2': 3, '99_3': 3, '99_6': 2, '99_8': 3, '99_9': 3, '99_10': 3, '100_1': 3, '100_3': 3, '100_4': 3, '104_2': 2, '104_3': 2, '104_4': 3, '115_1': 3, '115_2': 3, '115_3': 3, '115_4': 3, '115_5': 3, '115_6': 3, '115_7': 3, '115_8': 3, '115_9': 3, '115_10': 3, '125_1': 2, '125_2': 1, '125_3': 1, '125_5': 2, '125_6': 3, '125_8': 3, '125_9': 1, '125_10': 1, '131_3': 1, '131_5': 2, '131_6': 1, '131_8': 2, '143_1': 1, '143_2': 2, '143_5': 1, '145_1': 1, '145_2': 1, '145_3': 1, '145_4': 1, '150_1': 3, '150_2': 3, '150_3': 3, '150_4': 3, '150_5': 3, '152_1': 2, '152_4': 3, '154_1': 3, '154_2': 2, '154_3': 3, '154_4': 3, '154_5': 3, '154_7': 3, '154_8': 3, '154_9': 3, '154_10': 2, '157_7': 2, '157_9': 3, '157_10': 3, '157_14': 2, '157_16': 2, '159_1': 2, '159_2': 2, '159_3': 2, '159_4': 2, '159_5': 2, '159_6': 2, '159_7': 2, '159_9': 2, '162_1': 3, '162_3': 3, '162_4': 3, '162_6': 3, '165_1': 1, '165_2': 1, '165_3': 1, '179_1': 1, '179_2': 1, '179_4': 1, '179_5': 1, '179_6': 1, '181_1': 2, '181_2': 2, '181_3': 3, '181_4': 2, '181_5': 1, '181_7': 3, '181_8': 3, '181_9': 2, '190_2': 3, '190_6': 2, '190_8': 3, '190_10': 3, '190_12': 3, '190_14': 3, '194_1': 3, '199_1': 3, '199_2': 3, '199_3': 3, '207_1': 3, '207_3': 3, '209_1': 3, '209_2': 3, '209_5': 3, '209_6': 3, '209_7': 3, '209_8': 3, '209_9': 3, '209_12': 3, '209_13': 3, '209_14': 3, '209_15': 3, '209_18': 3, '212_1': 1, '212_2': 1, '212_4': 1, '212_5': 1, '212_6': 1, '212_7': 1, '212_9': 1, '212_10': 1, '212_11': 1, '212_12': 1, '212_13': 1, '212_14': 1, '222_4': 1, '222_5': 1, '222_7': 1, '222_10': 2, '222_11': 1, '222_12': 1, '222_13': 1, '222_15': 1, '222_16': 1, '222_17': 1, '222_18': 2, '237_1': 2, '237_2': 2, '237_3': 2, '237_5': 2, '237_6': 2, '239_1': 1, '239_2': 1, '239_3': 2, '239_4': 1, '239_5': 1, '239_7': 1, '239_8': 1, '251_1': 2, '251_2': 2, '251_3': 2, '251_5': 2, '252_1': 1, '254_1': 1, '254_2': 3, '254_6': 1, '268_1': 2, '268_2': 2, '268_3': 2, '268_4': 2, '268_5': 2, '268_6': 2, '268_7': 2, '274_1': 2, '274_2': 2, '274_3': 2, '274_4': 2, '274_5': 2, '274_6': 2, '274_7': 2, '274_10': 2, '284_1': 2, '284_2': 1, '284_3': 2, '284_4': 1, '284_5': 1, '284_6': 1, '284_7': 1, '284_8': 2, '284_9': 1, '284_10': 2, '284_11': 2, '284_12': 2, '289_1': 2, '289_2': 1, '289_3': 1, '289_4': 1, '289_5': 1, '289_6': 1, '289_7': 1, '289_8': 1, '289_9': 1, '289_10': 1, '289_11': 1, '289_12': 3, '289_13': 1, '289_14': 2, '289_15': 1, '289_16': 1, '289_17': 1, '289_18': 1, '289_19': 1, '289_20': 1, '289_21': 2, '289_22': 2, '289_23': 1, '289_24': 1, '289_25': 1, '289_26': 1, '289_27': 1, '289_28': 1, '289_29': 1, '289_30': 1, '289_31': 1, '289_32': 1, '289_33': 1, '289_34': 1, '289_35': 1, '289_36': 1, '289_37': 1, '289_38': 1, '289_39': 1, '289_40': 1, '289_41': 1, '289_42': 1, '289_43': 1, '289_44': 1, '289_45': 1, '289_46': 1, '289_47': 1, '289_48': 2, '289_49': 1, '289_50': 1, '289_51': 1, '289_52': 1, '289_53': 1, '289_54': 1, '289_55': 2, '289_56': 1, '289_57': 1, '289_58': 1, '289_59': 1, '289_60': 1, '289_61': 1, '289_62': 1, '289_63': 1, '289_64': 2, '289_65': 1, '289_66': 2, '289_67': 2, '289_68': 2, '294_1': 1, '294_2': 1, '294_3': 2, '294_4': 2, '294_5': 1, '294_6': 2, '294_7': 2, '294_8': 2, '294_9': 2, '294_10': 2, '294_11': 2, '294_12': 2, '294_14': 2, '294_15': 2, '294_16': 2, '294_17': 2, '294_18': 2, '294_19': 2, '294_20': 2, '294_21': 2, '294_22': 2, '294_23': 2, '294_24': 2, '294_25': 2, '294_26': 2, '294_27': 2, '294_28': 2, '294_29': 1, '294_30': 1, '294_31': 2, '294_32': 2, '294_33': 2, '294_34': 2, '294_35': 2, '294_36': 1, '294_37': 2, '294_38': 2, '294_39': 2, '294_40': 1, '294_41': 2, '294_42': 2, '294_43': 1, '294_44': 1, '294_45': 1, '294_46': 1, '294_47': 1, '294_48': 2, '298_1': 1, '298_2': 2, '298_3': 2, '298_4': 2, '298_5': 2, '298_6': 2, '298_7': 2, '298_8': 2, '298_9': 2, '298_10': 1, '298_11': 1, '298_12': 1, '298_13': 2, '298_14': 2, '298_15': 2, '298_16': 2, '298_17': 2, '298_18': 2, '298_19': 2, '298_20': 2, '298_21': 2, '298_22': 2, '298_23': 2, '298_24': 2, '298_25': 2, '298_26': 2, '298_27': 2, '298_28': 2, '298_29': 2, '298_30': 2, '298_31': 2, '298_32': 2, '298_33': 2, '298_34': 2, '298_35': 2, '298_36': 2, '298_37': 2, '298_38': 2, '298_39': 2, '298_40': 2, '298_41': 2, '298_42': 2, '298_43': 1, '298_44': 2, '298_45': 2, '298_46': 2, '298_47': 2, '298_48': 2, '298_49': 2, '298_50': 2, '298_51': 2, '298_52': 2, '298_53': 2, '298_54': 2, '298_55': 2, '298_56': 2, '298_57': 2, '298_58': 2, '298_59': 2, '305_1': 1, '305_2': 1, '305_3': 1, '306_1': 3, '306_2': 3, '306_3': 3, '306_4': 3, '306_5': 3, '306_6': 3, '306_7': 3, '306_8': 3, '306_9': 3, '306_10': 3, '306_11': 3, '306_12': 3, '306_13': 3, '306_14': 3, '306_15': 3, '306_16': 3, '306_17': 3, '306_18': 3, '306_19': 3, '306_20': 3, '306_21': 3, '308_1': 2, '308_2': 2, '308_3': 2, '308_4': 2, '308_5': 1, '308_6': 2, '308_7': 2, '308_8': 2, '308_9': 1, '308_10': 1, '314_2': 2, '314_3': 2, '314_4': 2, '314_5': 2, '320_1': 3, '320_2': 3, '320_3': 2, '320_4': 3, '320_5': 3, '320_6': 3, '320_7': 3, '320_8': 1, '320_9': 1, '320_10': 3, '320_11': 2, '320_12': 3, '320_13': 2, '320_14': 3, '320_15': 3, '320_16': 3, '320_17': 3, '323_1': 3, '323_2': 3, '323_3': 3, '323_4': 3, '325_1': 3, '325_2': 3, '325_3': 2, '325_4': 3, '325_5': 3, '325_6': 3, '325_7': 3, '325_8': 3, '325_9': 3, '325_10': 3, '325_11': 3, '325_12': 3, '325_13': 3, '325_14': 3, '325_15': 3, '325_16': 3, '325_17': 3, '325_18': 3, '325_19': 3, '325_20': 3, '325_21': 3, '325_22': 3, '325_23': 3, '325_24': 3, '325_25': 3, '325_26': 3, '325_27': 2, '325_28': 3, '325_29': 3, '325_30': 3, '325_31': 2, '325_32': 2, '325_33': 3, '325_34': 3, '325_35': 3, '325_36': 3, '325_37': 3, '325_38': 3, '325_39': 3, '325_40': 3, '325_41': 3, '325_42': 3, '325_43': 3, '325_44': 3, '325_45': 3, '325_46': 3, '325_47': 3, '325_48': 2, '325_49': 2, '325_50': 3, '325_51': 3, '325_52': 3, '325_53': 3, '325_54': 2, '325_55': 3, '325_56': 3, '325_57': 2, '332_1': 3, '332_2': 3, '332_3': 1, '333_1': 3, '333_2': 2, '333_3': 3, '333_4': 3, '335_1': 2, '335_2': 2, '335_3': 2, '335_4': 2, '335_5': 1, '335_6': 2, '335_7': 1, '335_8': 2, '335_9': 1, '335_10': 2, '342_1': 3, '346_1': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGS2rrUIZ_YX",
        "colab_type": "text"
      },
      "source": [
        "## Weighted Voting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZONQbr_aBpo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "b0e5bdc8-fae8-477b-ea42-bf50b0288077"
      },
      "source": [
        "y_pred_soft , y_true = predict(classifiers, sample_to_true_label, mode=\"weighted\", weights=[0.4, 0.5, 0.1])\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping unzipping files as input is a folder\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9eee6c2ae8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9eee6c2ae8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Skipping unzipping files as input is a folder\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 7\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9eee65b6a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9eee65b6a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Skipping unzipping files as input is a folder\n",
            "['1', '3', '2']\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/cs231n-emotiw/src/generators/pose_generator.py:135: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  x_new.append((lx[i] - origin_x) / len_x)\n",
            "/content/cs231n-emotiw/src/generators/pose_generator.py:136: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  x_new.append((ly[i] - origin_y) / len_y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9eee487950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9eee487950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Number of samples: 50\n",
            "[[0.18311593 0.27824757 0.53863645]\n",
            " [0.52522397 0.39609671 0.07867933]\n",
            " [0.28853178 0.24969785 0.46177036]]\n",
            "Predicted y-labels:\n",
            "[3 3 3 2 2 2 1 2 1 1 1 1 2 2 2 2 2 2 3 2 3 2 1 2 2 2 2 2 2 2 2 3 2 2 3 2 3\n",
            " 1 2 2 2 1 2 2 3 3 1 3 1 3]\n",
            "True y-labels:\n",
            "[3 3 3 1 1 1 1 3 1 2 1 3 2 2 2 2 2 1 1 1 1 1 2 2 1 2 2 2 2 2 2 3 3 3 1 3 1\n",
            " 1 1 2 1 1 1 2 2 1 1 3 1 3]\n",
            "Accuracy: 0.54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltPqItAaNNZK",
        "colab_type": "text"
      },
      "source": [
        "## Soft Voting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NczcqJnWpbjy",
        "colab_type": "code",
        "outputId": "bf5fb372-984a-45d6-ea24-15010d9d8405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "y_pred_soft , y_true = predict(classifiers, sample_to_true_label)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping unzipping files as input is a folder\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9ef137e598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9ef137e598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Skipping unzipping files as input is a folder\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 7\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9ef1327840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9ef1327840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Skipping unzipping files as input is a folder\n",
            "['1', '3', '2']\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/cs231n-emotiw/src/generators/pose_generator.py:135: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  x_new.append((lx[i] - origin_x) / len_x)\n",
            "/content/cs231n-emotiw/src/generators/pose_generator.py:136: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  x_new.append((ly[i] - origin_y) / len_y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9ef00db1e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9ef00db1e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Number of samples: 50\n",
            "[[0.18311593 0.27824757 0.53863645]\n",
            " [0.52522397 0.39609671 0.07867933]\n",
            " [0.28853178 0.24969785 0.46177036]]\n",
            "Predicted y-labels:\n",
            "[3 3 3 2 2 2 1 2 1 1 3 1 2 2 2 2 2 2 3 2 3 2 1 2 2 2 2 2 2 2 2 3 3 2 3 2 3\n",
            " 1 2 2 2 1 2 2 3 3 1 3 1 3]\n",
            "True y-labels:\n",
            "[3 3 3 1 1 1 1 3 1 2 1 3 2 2 2 2 2 1 1 1 1 1 2 2 1 2 2 2 2 2 2 3 3 3 1 3 1\n",
            " 1 1 2 1 1 1 2 2 1 1 3 1 3]\n",
            "Accuracy: 0.54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaB6nRK144XE",
        "colab_type": "text"
      },
      "source": [
        "## F1 Score -- Soft Voting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beA6fPJl47nB",
        "colab_type": "code",
        "outputId": "f502ad73-20a0-4b6d-b24a-08d831109aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_true, y_pred_soft, average='micro')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbo_4AnU40U1",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix -- Soft Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5awF-TCP4zdr",
        "colab_type": "code",
        "outputId": "c182fcb1-cd56-4cb2-ce4b-0d46a93cd921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "from sklearn import metrics \n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "cm=metrics.confusion_matrix(y_true,y_pred_soft)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "classes=['Pos' , 'Neu' , 'Neg'] \n",
        "\n",
        "y_pred_final , y_true_final = y_pred_soft - 1 , y_true - 1\n",
        "con_mat = tensorflow.math.confusion_matrix(labels=y_true_final, predictions=y_pred_final).numpy()\n",
        "\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                  index = classes, \n",
        "                  columns = classes)\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(11, 9))\n",
        "plt.title(\"Soft Voting Confusion Matrix with Scene & Audio\")\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9f0596a940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkXwFCN4NQ1D",
        "colab_type": "text"
      },
      "source": [
        "## Hard Voting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YnKQ-StNK6G",
        "colab_type": "code",
        "outputId": "c3dba5be-9c07-4f89-b09d-5d351979bd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "y_pred_hard , y_true = predict(classifiers, sample_to_true_label , mode=\"hard\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping unzipping files as input is a folder\n",
            "Skipping unzipping files as input is a folder\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 7\n",
            "Skipping unzipping files as input is a folder\n",
            "['1', '3', '2']\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/cs231n-emotiw/src/generators/pose_generator.py:135: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  x_new.append((lx[i] - origin_x) / len_x)\n",
            "/content/cs231n-emotiw/src/generators/pose_generator.py:136: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  x_new.append((ly[i] - origin_y) / len_y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 50\n",
            "Predicted y-labels:\n",
            "[3. 3. 3. 2. 2. 2. 1. 2. 1. 1. 3. 1. 1. 2. 1. 1. 2. 1. 3. 2. 3. 1. 1. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 3. 3. 1. 3. 1. 1. 1. 2. 2. 2. 1. 2. 2. 3. 1. 1. 1.\n",
            " 1. 1.]\n",
            "True y-labels:\n",
            "[3 3 3 1 1 1 1 3 1 2 1 3 2 2 2 2 2 1 1 1 1 1 2 2 1 2 2 2 2 2 2 3 3 3 1 3 1\n",
            " 1 1 2 1 1 1 2 2 1 1 3 1 3]\n",
            "Accuracy: 0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQwfjMTnmYEQ",
        "colab_type": "code",
        "outputId": "6437f832-262f-4be9-9fff-c1a57183a2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score(y_true, y_pred_hard, average='micro')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMc9KbQ6mqp8",
        "colab_type": "code",
        "outputId": "57dcacc7-70b7-47d6-e062-ad1b0a419946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "source": [
        "y_pred_final , y_true_final = y_pred_hard - 1 , y_true - 1\n",
        "con_mat = tensorflow.math.confusion_matrix(labels=y_true_final, predictions=y_pred_final).numpy()\n",
        "\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                  index = classes, \n",
        "                  columns = classes)\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(11, 9))\n",
        "plt.title(\"Hard Voting Confusion Matrix with Scene & Audio\")\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9efd959b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erlA2xY9WUnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}