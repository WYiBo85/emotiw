{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.0-la-tj-ak-ensemble_baseline_predictions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevincong95/cs231n-emotiw/blob/master/notebooks/2.0-la-tj-ak-ensemble_baseline_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rWTS99OI8l-",
        "colab_type": "text"
      },
      "source": [
        "## Video Sentiment Analysis in the Wild\n",
        "### Ensembling Notebook | CS231n\n",
        "\n",
        "This notebook preprocesses input videos to extract faces, frames, poses, and audio before running pre-trained models for each modality to predict group sentiment (positive, negative, or neutral). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58avLN7UlDDA",
        "colab_type": "code",
        "outputId": "96bb132c-e27a-450b-d13b-4b26bcc45fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Clone the code base\n",
        "!git clone 'https://github.com/kevincong95/cs231n-emotiw.git'\n",
        "\n",
        "# Switch to TF 1.x and navigate to the directory\n",
        "%tensorflow_version 1.x\n",
        "!pwd\n",
        "import os\n",
        "os.chdir('cs231n-emotiw')\n",
        "!pwd\n",
        "\n",
        "# Install required packages \n",
        "!pip install -r 'requirements-predictions.txt'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cs231n-emotiw' already exists and is not an empty directory.\n",
            "TensorFlow 1.x selected.\n",
            "/content\n",
            "/content/cs231n-emotiw\n",
            "Requirement already satisfied: tensorflow in /tensorflow-1.15.2/python3.6 (from -r requirements-predictions.txt (line 1)) (1.15.2)\n",
            "Requirement already satisfied: opencv-python>=4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 2)) (4.1.2.30)\n",
            "Requirement already satisfied: pydub==0.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 3)) (0.24.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 5)) (1.0.3)\n",
            "Requirement already satisfied: audioread in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 6)) (2.1.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 7)) (1.0.3)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 8)) (0.7)\n",
            "Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 11)) (1.18.4)\n",
            "Requirement already satisfied: openl3 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 12)) (0.3.1)\n",
            "Requirement already satisfied: SoundFile in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 13)) (0.10.3.post1)\n",
            "Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 14)) (0.2.6)\n",
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.6/dist-packages (from -r requirements-predictions.txt (line 16)) (19.18.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.29.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow->-r requirements-predictions.txt (line 1)) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow->-r requirements-predictions.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements-predictions.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-predictions.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (0.1.9)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.6/dist-packages (from moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements-predictions.txt (line 7)) (2018.9)\n",
            "Requirement already satisfied: kapre==0.1.4 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (0.1.4)\n",
            "Requirement already satisfied: scikit-image<0.15.0,>=0.14.3 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (0.14.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (1.4.1)\n",
            "Requirement already satisfied: keras<2.3.0,>=2.0.9 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (2.2.5)\n",
            "Requirement already satisfied: PySoundFile>=0.9.0.post1 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (0.9.0.post1)\n",
            "Requirement already satisfied: resampy<0.3.0,>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (0.2.2)\n",
            "Requirement already satisfied: h5py<3.0.0,>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from openl3->-r requirements-predictions.txt (line 12)) (2.10.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile->-r requirements-predictions.txt (line 13)) (1.14.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition->-r requirements-predictions.txt (line 15)) (7.0.0)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition->-r requirements-predictions.txt (line 15)) (0.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition->-r requirements-predictions.txt (line 15)) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->-r requirements-predictions.txt (line 1)) (46.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->-r requirements-predictions.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: librosa>=0.5 in /usr/local/lib/python3.6/dist-packages (from kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.16.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15.0,>=0.14.3->openl3->-r requirements-predictions.txt (line 12)) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15.0,>=0.14.3->openl3->-r requirements-predictions.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.15.0,>=0.14.3->openl3->-r requirements-predictions.txt (line 12)) (2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<2.3.0,>=2.0.9->openl3->-r requirements-predictions.txt (line 12)) (3.13)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy<0.3.0,>=0.2.1->openl3->-r requirements-predictions.txt (line 12)) (0.48.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile->-r requirements-predictions.txt (line 13)) (2.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5->kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.15.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.5->kapre==0.1.4->openl3->-r requirements-predictions.txt (line 12)) (0.22.2.post1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy<0.3.0,>=0.2.1->openl3->-r requirements-predictions.txt (line 12)) (0.31.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow->-r requirements-predictions.txt (line 1)) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14TqM3dYQ2wk",
        "colab_type": "text"
      },
      "source": [
        "#### Pose Pre-Requisites\n",
        "Pose extraction uses the [CMU OpenPose library](https://github.com/CMU-Perceptual-Computing-Lab/openpose) to extract body keypoints. We have pre-compiled this library for use in Colab but some system files still need to be installed. \n",
        "\n",
        "#### Retrieve the files\n",
        "\n",
        "The code block below demonstrates how to retrieve the files from GCS. However, feel free to skip this step if the files are already on the local disk or you have Google Drive mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qsmv0bdQJQp",
        "colab_type": "code",
        "outputId": "5f241433-4ac1-4933-ae3f-45530ffbad0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get -qq install -y libatlas-base-dev libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler libgflags-dev libgoogle-glog-dev liblmdb-dev opencl-headers ocl-icd-opencl-dev libviennacl-dev\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/openpose/openpose.tar.gz\n",
        "!tar -xzf openpose.tar.gz\n",
        "\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/train-tiny.zip\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/val-tiny.zip\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/test-tiny.zip\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/Train_labels.txt\n",
        "!wget https://storage.googleapis.com/cs231n-emotiw/data/Val_labels.txt\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 08:35:14--  https://storage.googleapis.com/cs231n-emotiw/openpose/openpose.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.206.128, 2a00:1450:400c:c02::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.206.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 598708936 (571M) [application/x-tar]\n",
            "Saving to: ‘openpose.tar.gz.2’\n",
            "\n",
            "openpose.tar.gz.2   100%[===================>] 570.97M  91.0MB/s    in 6.1s    \n",
            "\n",
            "2020-05-30 08:35:20 (93.7 MB/s) - ‘openpose.tar.gz.2’ saved [598708936/598708936]\n",
            "\n",
            "--2020-05-30 08:35:31--  https://storage.googleapis.com/cs231n-emotiw/data/train-tiny.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.184.128, 2a00:1450:400c:c02::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.184.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50131084 (48M) [application/zip]\n",
            "Saving to: ‘train-tiny.zip.2’\n",
            "\n",
            "train-tiny.zip.2    100%[===================>]  47.81M   144MB/s    in 0.3s    \n",
            "\n",
            "2020-05-30 08:35:32 (144 MB/s) - ‘train-tiny.zip.2’ saved [50131084/50131084]\n",
            "\n",
            "--2020-05-30 08:35:33--  https://storage.googleapis.com/cs231n-emotiw/data/val-tiny.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 2a00:1450:400c:c0b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48020782 (46M) [application/zip]\n",
            "Saving to: ‘val-tiny.zip.2’\n",
            "\n",
            "val-tiny.zip.2      100%[===================>]  45.80M   136MB/s    in 0.3s    \n",
            "\n",
            "2020-05-30 08:35:33 (136 MB/s) - ‘val-tiny.zip.2’ saved [48020782/48020782]\n",
            "\n",
            "--2020-05-30 08:35:34--  https://storage.googleapis.com/cs231n-emotiw/data/test-tiny.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 2a00:1450:400c:c0b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50596135 (48M) [application/zip]\n",
            "Saving to: ‘test-tiny.zip.2’\n",
            "\n",
            "test-tiny.zip.2     100%[===================>]  48.25M   164MB/s    in 0.3s    \n",
            "\n",
            "2020-05-30 08:35:35 (164 MB/s) - ‘test-tiny.zip.2’ saved [50596135/50596135]\n",
            "\n",
            "--2020-05-30 08:35:36--  https://storage.googleapis.com/cs231n-emotiw/data/Train_labels.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 2a00:1450:400c:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21653 (21K) [text/plain]\n",
            "Saving to: ‘Train_labels.txt.2’\n",
            "\n",
            "Train_labels.txt.2  100%[===================>]  21.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-30 08:35:36 (58.9 MB/s) - ‘Train_labels.txt.2’ saved [21653/21653]\n",
            "\n",
            "--2020-05-30 08:35:37--  https://storage.googleapis.com/cs231n-emotiw/data/Val_labels.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 2a00:1450:400c:c0b::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6265 (6.1K) [text/plain]\n",
            "Saving to: ‘Val_labels.txt.2’\n",
            "\n",
            "Val_labels.txt.2    100%[===================>]   6.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-30 08:35:38 (63.5 MB/s) - ‘Val_labels.txt.2’ saved [6265/6265]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmTc8EMiFFzt",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess Files\n",
        "\n",
        "Here, we will instantiate each of the preprocessors and process all of the input video files.\n",
        "\n",
        "NOTE: Change the input parameters as needed.\n",
        "\n",
        "WARNING: This may take several hours to complete, depending on the number of files.\n",
        "\n",
        "In general, pre-processing will extract the following:\n",
        "- Video frames\n",
        "- Pose keypoints\n",
        "- Faces from each video frame\n",
        "- Audio waveform and audio features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36LTqWyJFHGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a17b1e52-4066-4e6a-e732-a0923c0c45f3"
      },
      "source": [
        "from src.preprocessors.preprocess_all_modes import preprocess\n",
        "from src.preprocessors.pose_preprocessor import PosePreprocessor\n",
        "\n",
        "print(\"Starting to preprocess train data\")\n",
        "preprocess(video_folder=\"train-tiny.zip\", label_file=\"Train_labels.txt\", local_base_path=\"train-tiny\")\n",
        "\n",
        "# print(\"Starting to preprocess val data\")\n",
        "# preprocess(video_folder=\"val-tiny.zip\", label_file=\"Val_labels.txt\", local_base_path=\"val-tiny\")\n",
        "\n",
        "# print(\"Starting to preprocess test data\")\n",
        "# preprocess(video_folder=\"test-tiny.zip\", local_base_path=\"test-tiny\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting to preprocess train data\n",
            "Video Preprocessor created with video_folder = train-tiny.zip , label_file = Train_labels.txt , output_folder = train-tiny-frames, output_file = train-tiny-frames.zip\n",
            "Frames will be created with height = 320 , width = 480 , sample_every = 10\n",
            "Video Preprocessor created with video_folder = train-tiny.zip , output_folder = train-tiny-faces, output_file = train-tiny-faces.zip\n",
            "Frames will be created with height = 320 , width = 480 , sample_every = 10\n",
            "Pose Preprocessor created with is_test = False, video_frame_folder = train-tiny-frames , output_folder = train-tiny-pose, output_file = train-tiny-pose.zip\n",
            "Video Preprocessor created with video_folder = train-tiny.zip , output_folder = train-tiny-audio, output_file = train-tiny-audio.zip\n",
            "Frames will be created with hop_size = 0.5\n",
            "Unzipping files to temp dir train-tiny-frames_tmp...\n",
            "Finished unzipping files\n",
            "Found 50 videos\n",
            "Processing video 1/50 with name 101_12.mp4 and class 3 \n",
            "\n",
            "Processing video 2/50 with name 101_30.mp4 and class 1 \n",
            "\n",
            "Processing video 4/50 with name 112_5.mp4 and class 2 \n",
            "\n",
            "Processing video 6/50 with name 122_1.mp4 and class 1 \n",
            "\n",
            "Processing video 8/50 with name 140_1.mp4 and class 1 \n",
            "\n",
            "Processing video 10/50 with name 16_14.mp4 and class 1 \n",
            "\n",
            "Processing video 12/50 with name 188_22.mp4 and class 3 \n",
            "\n",
            "Processing video 14/50 with name 198_5.mp4 and class 3 \n",
            "\n",
            "Processing video 16/50 with name 217_15.mp4 and class 1 \n",
            "\n",
            "Processing video 20/50 with name 276_3.mp4 and class 2 \n",
            "\n",
            "Processing video 18/50 with name 220_6.mp4 and class 3 \n",
            "\n",
            "Processing video 22/50 with name 277_3.mp4 and class 2 \n",
            "\n",
            "Processing video 24/50 with name 281_2.mp4 and class 2 \n",
            "\n",
            "Processing video 26/50 with name 2_2.mp4 and class 3 \n",
            "\n",
            "Processing video 30/50 with name 321_15.mp4 and class 3 \n",
            "\n",
            "Processing video 28/50 with name 303_41.mp4 and class 3 \n",
            "\n",
            "Processing video 32/50 with name 324_34.mp4 and class 2 \n",
            "\n",
            "Processing video 5/50 with name 119_8.mp4 and class 3 \n",
            "\n",
            "Processing video 9/50 with name 148_4.mp4 and class 3 \n",
            "\n",
            "Processing video 13/50 with name 197_12.mp4 and class 1 \n",
            "\n",
            "Processing video 17/50 with name 217_3.mp4 and class 1 \n",
            "\n",
            "Processing video 21/50 with name 276_8.mp4 and class 1 \n",
            "\n",
            "Processing video 25/50 with name 286_4.mp4 and class 2 \n",
            "\n",
            "Processing video 29/50 with name 312_1.mp4 and class 2 \n",
            "\n",
            "Processing video 11/50 with name 188_15.mp4 and class 3 \n",
            "\n",
            "Processing video 3/50 with name 108_13.mp4 and class 3 \n",
            "\n",
            "Processing video 19/50 with name 258_2.mp4 and class 2 \n",
            "\n",
            "Processing video 27/50 with name 300_56.mp4 and class 3 \n",
            "\n",
            "Processing video 23/50 with name 280_12.mp4 and class 1 \n",
            "\n",
            "Processing video 15/50 with name 204_13.mp4 and class 3 \n",
            "\n",
            "Processing video 31/50 with name 321_31.mp4 and class 1 \n",
            "\n",
            "Processing video 7/50 with name 133_2.mp4 and class 1 \n",
            "\n",
            "Processing video 33/50 with name 324_56.mp4 and class 3 \n",
            "\n",
            "Processing video 34/50 with name 324_96.mp4 and class 3 \n",
            "\n",
            "Processing video 35/50 with name 328_13.mp4 and class 3 \n",
            "\n",
            "Processing video 36/50 with name 334_21.mp4 and class 3 \n",
            "\n",
            "Processing video 37/50 with name 33_20.mp4 and class 1 \n",
            "\n",
            "Processing video 38/50 with name 34_9.mp4 and class 2 \n",
            "\n",
            "Processing video 39/50 with name 3_17.mp4 and class 3 \n",
            "\n",
            "Processing video 40/50 with name 3_29.mp4 and class 3 \n",
            "\n",
            "Processing video 41/50 with name 41_18.mp4 and class 1 \n",
            "\n",
            "Processing video 42/50 with name 43_12.mp4 and class 1 \n",
            "\n",
            "Processing video 43/50 with name 52_1.mp4 and class 2 \n",
            "\n",
            "Processing video 44/50 with name 61_15.mp4 and class 1 \n",
            "\n",
            "Processing video 45/50 with name 64_5.mp4 and class 1 \n",
            "\n",
            "Processing video 46/50 with name 68_9.mp4 and class 1 \n",
            "\n",
            "Processing video 47/50 with name 69_30.mp4 and class 2 \n",
            "\n",
            "Processing video 48/50 with name 7_6.mp4 and class 1 \n",
            "\n",
            "Processing video 49/50 with name 80_1.mp4 and class 1 \n",
            "\n",
            "Processing video 50/50 with name 97_22.mp4 and class 3 \n",
            "\n",
            "***** Submitted all tasks *****\n",
            "***** Completed *****\n",
            "Starting to zip files to train-tiny-frames.zip\n",
            "Done zipping files to train-tiny-frames.zip\n",
            "Done!\n",
            "Skipping unzipping files as input is a folder\n",
            "Starting pose extraction for train-tiny-frames\n",
            "Starting pose extraction for train-tiny-frames/1\n",
            "\n",
            "build/examples/openpose/openpose.bin: error while loading shared libraries: libopenpose.so.1.6.0: cannot open shared object file: No such file or directory\n",
            "\n",
            "Starting pose extraction for train-tiny-frames/3\n",
            "\n",
            "build/examples/openpose/openpose.bin: error while loading shared libraries: libopenpose.so.1.6.0: cannot open shared object file: No such file or directory\n",
            "\n",
            "Starting pose extraction for train-tiny-frames/2\n",
            "\n",
            "build/examples/openpose/openpose.bin: error while loading shared libraries: libopenpose.so.1.6.0: cannot open shared object file: No such file or directory\n",
            "\n",
            "Starting to zip files to train-tiny-pose.zip\n",
            "Done zipping files to train-tiny-pose.zip\n",
            "Done!\n",
            "Unzipping files to temp dir train-tiny-audio_tmp...\n",
            "Finished unzipping files\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 101_12.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 101_30.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 108_13.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 112_5.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 119_8.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 122_1.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 133_2.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 140_1.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 148_4.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 16_14.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 188_15.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 188_22.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 197_12.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 198_5.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 204_13.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 217_15.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 217_3.mp4extracted_audio.wav ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "187/187 [==============================] - 3s 15ms/step\n",
            "(17, 11, 6144)\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 220_6.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 258_2.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 276_3.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 276_8.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 277_3.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 280_12.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 281_2.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 286_4.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 2_2.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 300_56.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 303_41.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 312_1.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 321_15.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 321_31.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 324_34.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 324_56.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 324_96.mp4extracted_audio.wav ...\n",
            "187/187 [==============================] - 1s 4ms/step\n",
            "(34, 11, 6144)\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 328_13.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 334_21.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 33_20.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 34_9.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 3_17.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 3_29.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 41_18.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 43_12.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 52_1.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 61_15.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 64_5.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 68_9.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 69_30.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 7_6.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 80_1.mp4extracted_audio.wav ...\n",
            "Moviepy - Running:\n",
            ">>> \"+ \" \".join(cmd)\n",
            "Moviepy - Command successful\n",
            "Reading file 97_22.mp4extracted_audio.wav ...\n",
            "176/176 [==============================] - 1s 6ms/step\n",
            "(50, 11, 6144)\n",
            "Starting to zip files to train-tiny-audio.zip\n",
            "Done zipping files to train-tiny-audio.zip\n",
            "Done!\n",
            "Done all pre-processing. Folders created: \n",
            "   train-tiny-frames\n",
            "   train-tiny-faces\n",
            "   train-tiny-pose\n",
            "   train-tiny-audio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc0OOW4vVI7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the openpose folder as it is no longer required\n",
        "!rm -rf openpose/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEaDQAsS8XQ",
        "colab_type": "text"
      },
      "source": [
        "### Run Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0_gfpBjXP6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPXBx950Xd8z",
        "colab_type": "code",
        "outputId": "fcc798cd-edc3-4770-d475-7b2a39d454ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuMdFr8GT8uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a27c416-bb07-4f81-c8bd-b6db524d4bff"
      },
      "source": [
        "!pwd\n",
        "import os\n",
        "os.chdir('cs231n-emotiw')\n",
        "!pwd\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/cs231n-emotiw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRW9Kl4bgA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b95a16cd-0b49-43d0-8a4b-305e5483c706"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/18)\u001b[K\rremote: Counting objects:  11% (2/18)\u001b[K\rremote: Counting objects:  16% (3/18)\u001b[K\rremote: Counting objects:  22% (4/18)\u001b[K\rremote: Counting objects:  27% (5/18)\u001b[K\rremote: Counting objects:  33% (6/18)\u001b[K\rremote: Counting objects:  38% (7/18)\u001b[K\rremote: Counting objects:  44% (8/18)\u001b[K\rremote: Counting objects:  50% (9/18)\u001b[K\rremote: Counting objects:  55% (10/18)\u001b[K\rremote: Counting objects:  61% (11/18)\u001b[K\rremote: Counting objects:  66% (12/18)\u001b[K\rremote: Counting objects:  72% (13/18)\u001b[K\rremote: Counting objects:  77% (14/18)\u001b[K\rremote: Counting objects:  83% (15/18)\u001b[K\rremote: Counting objects:  88% (16/18)\u001b[K\rremote: Counting objects:  94% (17/18)\u001b[K\rremote: Counting objects: 100% (18/18)\u001b[K\rremote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 10 (delta 8), reused 10 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects:  10% (1/10)   \rUnpacking objects:  20% (2/10)   \rUnpacking objects:  30% (3/10)   \rUnpacking objects:  40% (4/10)   \rUnpacking objects:  50% (5/10)   \rUnpacking objects:  60% (6/10)   \rUnpacking objects:  70% (7/10)   \rUnpacking objects:  80% (8/10)   \rUnpacking objects:  90% (9/10)   \rUnpacking objects: 100% (10/10)   \rUnpacking objects: 100% (10/10), done.\n",
            "From https://github.com/kevincong95/cs231n-emotiw\n",
            "   b05e030..be272a2  master     -> origin/master\n",
            "Updating b05e030..be272a2\n",
            "Fast-forward\n",
            " .DS_Store                                          |  Bin \u001b[31m8196\u001b[m -> \u001b[32m10244\u001b[m bytes\n",
            " ....0-la-tj-ak-ensemble_baseline_predictions.ipynb | 1331 \u001b[31m--------------------\u001b[m\n",
            " notebooks/image_caption/.DS_Store                  |  Bin \u001b[31m6148\u001b[m -> \u001b[32m6148\u001b[m bytes\n",
            " src/classifiers/audio_classifier.py                |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " src/classifiers/frames_classifier.py               |    4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " src/classifiers/pose_classifier.py                 |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 6 files changed, 4 insertions(+), 1335 deletions(-)\n",
            " delete mode 100644 notebooks/2.0-la-tj-ak-ensemble_baseline_predictions.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTGjCxWxklX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab1c9bc1-f3cb-4e03-cda3-4cd326661ab5"
      },
      "source": [
        "!ls -l train-tiny-audio"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 2 root root 4096 May 30 08:18 audio-pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOIbnIoH_AHV",
        "colab_type": "code",
        "outputId": "dfbf689a-680b-49f1-afe2-11d202cb1fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "from src.classifiers.audio_classifier import AudioClassifier\n",
        "from src.classifiers.frames_classifier import FramesClassifier\n",
        "from src.classifiers.pose_classifier import PoseClassifier\n",
        "from src.classifiers.utils import get_num_samples\n",
        "import numpy as np\n",
        "\n",
        "audio_classifier = AudioClassifier('train-tiny-audio', model_location='https://storage.googleapis.com/cs231n-emotiw/models/OPENL3_audio_api_train_test-1-500-epochs-0.5_hop--BEST_MODEL-w-VAL--1e-6-lr-0.2-dropout-512-feat-map-batch-norm-3-cnn-layers.h5', is_test=False)\n",
        "frames_classifier = FramesClassifier('train-tiny-frames', model_location='https://storage.googleapis.com/cs231n-emotiw/models/frame-classifier-resnet-lstm-x3.h5', is_test=False)\n",
        "\n",
        "# TODO: Does not currently work due to problem with OpenPose feature extraction\n",
        "# pose_classifier = PoseClassifier('train-tiny-pose', model_location='https://storage.googleapis.com/cs231n-emotiw/models/pose-classifier-v5.h5', is_test=False)\n",
        "\n",
        "classifiers = [audio_classifier, frames_classifier]\n",
        "\n",
        "#\n",
        "# MAX CLASSIFIER\n",
        "#\n",
        "classifier_outputs = []\n",
        "classifier_samples = []\n",
        "sample_to_row = {}\n",
        "num_samples = 0\n",
        "\n",
        "for c, classifier in enumerate(classifiers):\n",
        "    results, samples = classifier.predict()\n",
        "    classifier_outputs.append(results.tolist())\n",
        "    classifier_samples.append(list(samples))\n",
        "    num_samples = len(list(samples))\n",
        "    print(num_samples)\n",
        "\n",
        "for i, sample in enumerate(classifier_samples[0]):\n",
        "    sample_to_row[sample] = i\n",
        "\n",
        "X = np.zeros(shape=(num_samples, len(classifiers) * 3))\n",
        "for c, output in enumerate(classifier_outputs):\n",
        "    samples = classifier_samples[c]\n",
        "    for i, row in enumerate(output):\n",
        "        sample = samples[i]\n",
        "        X[sample_to_row[sample], np.argmax(row).item()] += 1\n",
        "\n",
        "print(np.argmax(X, axis=1))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AudioClassifier created with audio_folder = train-tiny-audio , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/OPENL3_audio_api_train_test-1-500-epochs-0.5_hop--BEST_MODEL-w-VAL--1e-6-lr-0.2-dropout-512-feat-map-batch-norm-3-cnn-layers.h5\n",
            "FramesClassifier created with frames_folder = train-tiny-frames , is_test = False , model_location = https://storage.googleapis.com/cs231n-emotiw/models/frame-classifier-resnet-lstm-x3.h5\n",
            "Skipping unzipping files as input is a folder\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f169a4c2598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f169a4c2598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "50\n",
            "Skipping unzipping files as input is a folder\n",
            "Found 50 frames belonging to 50 videos belonging to 3 classes.\n",
            "Min frames determined to be 13\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f1696b59bf8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f1696b59bf8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "50\n",
            "[0 0 2 1 1 1 0 0 0 0 2 2 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 2 0 2 1 0 0 1 0 0 0\n",
            " 0 2 2 0 0 1 1 1 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xwIeRF_x40m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(mp4_dir , train_target_path , model_list= [model,] , model_paths=[\"\",] , num_models = 1 , mode=\"soft\" , complexFusion=False):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Notes: \n",
        "\n",
        "  All file preprocessing will occur in this function.\n",
        "\n",
        "\n",
        "  Inputs\n",
        "  \n",
        "  * mp4_train_dir - The directory of .mp4 file paths that the model will make predictions from.\n",
        "  * train_target_path - The target path. Should be in .txt format.\n",
        "  * A list of pretrained models that will be used in the ensemble during prediction\n",
        "\n",
        "\n",
        "  Outputs \n",
        "\n",
        "  * An array of size (M) where M is the number of samples\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  assert len(model_list) == num_models\n",
        "  assert len(model_paths) == num_models\n",
        "\n",
        "  all_model_predictions = []\n",
        "  X_list = []\n",
        "\n",
        "  counter = 0\n",
        "  \n",
        "                            \n",
        "\n",
        "  for model in models:\n",
        "\n",
        "    model.load_model(model_paths[counter])\n",
        "\n",
        "\n",
        "    X = X_list[counter]\n",
        "    \n",
        "   \n",
        "\n",
        "    all_model_predictions.append(model.predict(dir=\"video path/*.mp4\" , X=X)) # Predict returns an (M , 3) array\n",
        "    \n",
        "    counter += 1\n",
        "\n",
        "  all_model_predictions = np.asarray(all_model_predictions , dtype='float32') # (num_models , M , 3)\n",
        "\n",
        "\n",
        "  assert mode in [\"soft\" , \"hard\"]\n",
        "\n",
        "  if mode == \"soft\":\n",
        "\n",
        "    \n",
        "    # Take the average of each \n",
        "\n",
        "    predictions = np.mean(all_model_predictions , axis=0)\n",
        "\n",
        "    predictions = np.argmax(predictions , axis = 1)\n",
        "\n",
        "    return predictions  # (M,) in the domain [0,1,2]\n",
        "\n",
        "  # TODO: Majority vote\n",
        "\n",
        "  positive_hard = np.argmax(positive_arr , axis=1) # ()\n",
        "\n",
        "\n",
        "\n",
        "  return predictions "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TJCeo420Zx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(predictions, targets):\n",
        "\n",
        "  # TO DO: Add evaluate code here\n",
        "\n",
        "  \"\"\"\n",
        "  Inputs\n",
        "  \n",
        "  * predictions np array of shape M where M is the number of samples\n",
        "  * target array of shape M where M is the number of samples\n",
        "\n",
        "\n",
        "  Outputs \n",
        "\n",
        "  * Model accuracy scalar \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  assert len(targets) == len(predictions)\n",
        "\n",
        "  incorrect = np.count_nonzero(predictions - targets)\n",
        "\n",
        "  acc = (targets.shape[0] - incorrect) / targets.shape[0]\n",
        "\n",
        "  return acc\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}