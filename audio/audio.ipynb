{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtMIoO7CwrPW",
        "colab_type": "code",
        "outputId": "6d01142e-debe-4f90-afd0-4c0ede0afbdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' , force_remount=True)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C2i_RzuxKXn",
        "colab_type": "code",
        "outputId": "e91cee28-fade-4cdf-edad-bb7a34e1f3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pydub"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (0.24.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdjKfwTntt5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports \n",
        "from tensorflow.keras.models import load_model\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_audio\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import importlib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "os.system(\"pip install pydub\")\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/notebooks/audio-new')\n",
        "\n",
        "\n",
        "import arffToNp\n",
        "importlib.reload(arffToNp)\n",
        "import subprocess\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPKtwAdgQzil",
        "colab_type": "text"
      },
      "source": [
        "# Audio API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOSIfnb4PRuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# TODO: add option for soft vs hard\n",
        "def predict(mp4_filepath, best_model_filepath):\n",
        "    \"\"\"\n",
        "    Outputs:\n",
        "    - A tuple with predictions for each class (positive, neutral, negative)\n",
        "    \"\"\"\n",
        "\n",
        "    model = fer_model()\n",
        "    model.load_model(best_model_filepath)\n",
        "    return model.predict(mp4_filepath)\n",
        "\n",
        "class audio_model:\n",
        "    def __init__(self):\n",
        "        self.model = ()\n",
        "        return\n",
        "\n",
        "    def predict(self, mp4_filepath):\n",
        "        self.preprocess(mp4_filepath)\n",
        "        X = cv2.imload(\"test/happy.jpg\")\n",
        "        X = cv2.resize(X, (48,48))\n",
        "        X = cv2.cvtColor(X, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        return self.model.predict(img)\n",
        "        #return (0.1,0.2,0.7)\n",
        "\n",
        "    def load_model(self, best_model_filepath):\n",
        "        self.model = load_model(best_model_filepath)\n",
        "        return\n",
        "\n",
        "    def train(self, X_train , y_train):\n",
        "\n",
        "\n",
        "         # train the model\n",
        "              # TO DO : Implement a recurrent network for each \"picture\"\n",
        "        # Look into autoencoders/PCA for dim reduction\n",
        "\n",
        "        # Reccurent network on row time steps\n",
        "        inputs = keras.Input(shape=[216,128])\n",
        "\n",
        "        recurrent_1 = keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 128] , dropout=0.2 , activation='selu') #A sequence of any length with dimensions 128 (i.e. 128 columns)\n",
        "\n",
        "        recurrent_2 = keras.layers.LSTM(32)\n",
        "\n",
        "        # We use the row of the image at each time step. That's why we have a repeat \n",
        "        # vector layer.\n",
        "        #repeat_vector = keras.layers.RepeatVector(216, input_shape=[30])\n",
        "        #recurrent_3 = keras.layers.LSTM(100, return_sequences=False)\n",
        "        dense_1 = keras.layers.Dense(32 , activation='selu')\n",
        "        softmax = keras.layers.Dense(3 , activation='softmax')\n",
        "\n",
        "        x = recurrent_1(inputs)\n",
        "        x = recurrent_2(x)\n",
        "\n",
        "\n",
        "        #x = repeat_vector(x)\n",
        "        #x = recurrent_3(x)\n",
        "        x = dense_1(x)\n",
        "\n",
        "\n",
        "        outputs = softmax(x)\n",
        "\n",
        "        rnn_ae = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "          initial_learning_rate=1e-5,\n",
        "          decay_steps=10000,\n",
        "          decay_rate=0.9)\n",
        "        opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "        rnn_ae.compile(loss='sparse_categorical_crossentropy' , optimizer=opt , metrics=['accuracy'])\n",
        "        rnn_ae.fit(X_train , y_train , epochs=500 , batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "        return rnn_ae\n",
        "\n",
        "    def preprocess(self, mp4_filepath , target_label_path=None):\n",
        "\n",
        "      \"\"\"\n",
        "      Outputs:\n",
        "      - A numpy array with dimensions (m,n). \n",
        "        - m is the units in time dependent on the audio splice rate.\n",
        "        - n is the number of features from the openSMILE library.\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "      output_wav_file = mp4_filepath[-5] + 'extracted_audio.wav'\n",
        "      mp4_filename = os.path.basename(mp4_filepath)\n",
        "      audio_home_dir = os.path.dirname(mp4_filepath)\n",
        "      os.chdir(audio_home_dir)\n",
        "\n",
        "      # Strip the audio from video and store as .wav file\n",
        "      ffmpeg_extract_audio(mp4_filepath, output_wav_file)\n",
        "      !cd '$audio_home_dir' ; mkdir to_zip\n",
        "\n",
        "      # splice the audio files into 2 seconds with 100 ms sliding window.\n",
        "      # 30 kHz sampling rate\n",
        "      !cd '$audio_home_dir' ; python SliceAudio.py -i *.wav -o wav -c 2  -b 2 -s 30000 -w 100 -l 2000\n",
        "    \n",
        "      # Zip and move files from drive to vm\n",
        "      !cd '$audio_home_dir'  ; zip -r -qq to_zip.zip  to_zip ; cd '$audio_home_dir' ; mv 'to_zip.zip' '/content/' \n",
        "\n",
        "      # Remove the old zip folder in vm\n",
        "      !cd '$audio_home_dir' ; cd to_zip ; rm *.wav  ; cd - ; rm -d to_zip\n",
        "\n",
        "      # Inflate the zip folder in vm\n",
        "      !cd '/content/' ; unzip -qq to_zip.zip \n",
        "\n",
        "\n",
        "      # OpenSMILE feature extraction\n",
        "      out_fn = os.path.join('/content/openSmile-features.arff')\n",
        "      os.chdir('/content/to_zip/')\n",
        "      aligned_files = glob.glob('*.wav')\n",
        "      os.chdir('/content/')\n",
        "      for in_fn in aligned_files:\n",
        "        in_fn = os.path.join('/content/to_zip/' , in_fn)\n",
        "        name = os.path.basename(in_fn)\n",
        "        !cd 'opensmile-2.3.0' ; inst/bin/SMILExtract -C config/IS13_ComParE.conf -I '$in_fn' -O '$out_fn' -N $name\n",
        "\n",
        "      # Convert .arff to .csv\n",
        "      all_timepoints_feature_array = arffToNp.convert(out_fn , '')\n",
        "      print(\"The shape of the feature matrix for one \\n video is: \" , all_timepoints_feature_array.shape)\n",
        "\n",
        "      # Clean up            \n",
        "      !cd to_zip ; rm *.wav ; cd - ; rm -d to_zip      \n",
        "      os.remove(out_fn)\n",
        "      !cd '$audio_home_dir' ; rm *.wav ; rm *.csv\n",
        "      !rm to_zip.zip\n",
        "      !rm *.csv\n",
        "      !rm *.arff\n",
        "\n",
        "\n",
        "      # Standardize\n",
        "      scaler = StandardScaler()\n",
        "      all_timepoints_feature_array = scaler.fit_transform(all_timepoints_feature_array)\n",
        "\n",
        "\n",
        "      # Get the Y values \n",
        "      target = None \n",
        "      if target_label_path is not None:\n",
        "        target_labels = np.genfromtxt(target_label_path , delimiter = ' ' , dtype='str')\n",
        "        target_index = np.where(target_labels[: , 0] == mp4_filename[:-4])[0]\n",
        "        target = int(target_labels[: , 1][target_index])\n",
        "\n",
        "\n",
        "\n",
        "      return all_timepoints_feature_array , target\n",
        "\n",
        "\n",
        "      # Read in each video file and add the (m,n) feature matrix to a 3D array\n",
        "\n",
        "    def get_feature_batch(self, input_files_dir , batch_size=3000 , target_label_path=None):\n",
        "        \"\"\"\n",
        "        Inputs: \n",
        "        - Path to the .mp4 files\n",
        "        Outputs:\n",
        "        - An ndarray with dims (s , m , n)\n",
        "          - s is the number of samples\n",
        "          - m is the number of slices for that sample (32)\n",
        "          - n is the number of features (6373)\n",
        "        \"\"\"\n",
        "\n",
        "        output_x = []\n",
        "        output_y = None\n",
        "\n",
        "        if target_label_path is not None:\n",
        "          output_y = []\n",
        "\n",
        "\n",
        "        counter = 0\n",
        "\n",
        "        fileList = glob.glob(input_files_dir + '*.mp4')\n",
        "\n",
        "        print(fileList)\n",
        "\n",
        "      \n",
        "        for file_path in fileList:\n",
        "         \n",
        "          print(file_path)\n",
        "\n",
        "          one_sample_feat_matrix , y = self.preprocess(file_path , target_label_path=target_label_path)\n",
        "          output_x.append(one_sample_feat_matrix)\n",
        "          if target_label_path is not None:            \n",
        "            output_y.append(y)\n",
        "\n",
        "          if counter >= batch_size:\n",
        "            break\n",
        "\n",
        "          counter += 1\n",
        "\n",
        "        output_y = np.asarray(output_y)\n",
        "        output_y -= 1\n",
        "\n",
        "        return output_x , output_y\n",
        "\n",
        "def installOpenSMILE():\n",
        "    \"\"\"\n",
        "    You must upload your downloaded version of openSMILE from the site to \n",
        "    cloud.\n",
        "\n",
        "    \"\"\"\n",
        "    os.chdir('/content/')\n",
        "    !tar -zxvf 'opensmile-2.3.0.tar.gz'\n",
        "    !sed -i '117s/(char)/(unsigned char)/g' opensmile-2.3.0/src/include/core/vectorTransform.hpp\n",
        "    !sudo apt-get update\n",
        "    !sudo apt-get install autoconf automake libtool m4 gcc\n",
        "    !cd 'opensmile-2.3.0' ; bash buildStandalone.sh\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCmJo14bwJPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "installOpenSMILE()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuQ7ZKNaErLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd 'opensmile-2.3.0' ; inst/bin/SMILExtract -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz4-Eq7s1b8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_model_1 = audio_model()\n",
        "target_path =  '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/datasets/emotiw/Val_labels.txt'\n",
        "output_arr , y = audio_model_1.preprocess(mp4_filepath='/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/notebooks/audio-new/1_1.mp4' , target_label_path=target_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uvfF-vBuWFx",
        "colab_type": "code",
        "outputId": "d1ef2e35-70ff-4320-cc93-deba92b56a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!cd '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/notebooks/audio-new/'; ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1_1.mp4\t\t\t\t  audio.ipynb\t\t  openSMILEShell.sh\n",
            "1extracted_audio.wav\t\t  audio.py\t\t  __pycache__\n",
            "arffToNp.py\t\t\t  opensmile-2.3.0\t  SliceAudio.py\n",
            "audio-deep-learning-models.ipynb  opensmile-2.3.0.tar.gz  to_zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpSBR3iwqBCu",
        "colab_type": "code",
        "outputId": "cd136dda-a8e4-4ad5-dc2b-c341413338fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output_arr.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 6373)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qclXsD0rQurr",
        "colab_type": "text"
      },
      "source": [
        "# Set up Processing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO6kw5xPczMj",
        "colab_type": "code",
        "outputId": "8095ca28-acd9-43b0-e161-1cdd42c959d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Imports\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: to_zip: No such file or directory\n",
            "rm: cannot remove 'to_zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XjbNuXMRCYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing Pipeling\n",
        "home_dir = '/content/'\n",
        "!cd '$home_dir'; mkdir train_vids ; mkdir val_vids\n",
        "train_path = '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/datasets/emotiw/Train.zip'\n",
        "train_target_path =  '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/datasets/emotiw/Train_labels.txt'\n",
        "\n",
        "val_path = '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/datasets/emotiw/Val.zip'\n",
        "val_target_path =  '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/datasets/emotiw/Val_labels.txt'\n",
        "\n",
        "vm_train_path = '/content/train_vids'\n",
        "vm_val_path = '/content/val_vids'\n",
        "slice_audio_path = '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/notebooks/audio-new/SliceAudio.py'\n",
        "\n",
        "\n",
        "# Copy files to vm and inflate\n",
        "!cp '$train_path' '$vm_train_path' ; cd '$vm_train_path' ; unzip -qq 'Train.zip'\n",
        "!cp '$val_path' '$vm_val_path' ; cd '$vm_val_path' ; unzip -qq 'Val.zip'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbiO9Y8pXN0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " !cd '$vm_val_path' ; unzip 'Val.zip'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd8RXFbQX__G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80964f20-781b-49ed-d15f-42303b5da35a"
      },
      "source": [
        "inflated_train_path = '/content/train_vids/Train/'\n",
        "inflated_val_path = '/content/train_vids/Val/'\n",
        "\n",
        "# Copy SliceAudio to dirs\n",
        "!cp '$slice_audio_path' '$inflated_train_path' ; cp '$slice_audio_path' '$inflated_val_path'"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create regular file '/content/train_vids/Val/': Not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfhZzwnkh3pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_model_test = audio_model()\n",
        "      \n",
        "X , y = audio_model_test.get_feature_batch(inflated_train_path , batch_size=3 , target_label_path=train_target_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiBhbVOuqCSf",
        "colab_type": "code",
        "outputId": "a7dce242-e9ef-42fe-eaae-51ed96ef1eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#final_array_1 = np.asarray(batched_array , dtype='float')\n",
        "#!cd '/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/notebooks/audio-new/arrays'\n",
        "X = np.asarray(X , dtype = 'float32')\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q__yJA75bhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/Challenge-Data/emotiw/models/audio.train-opensmile-standardized_X.all.pickle', 'wb') as f:\n",
        "    pickle.dump(X, f)\n",
        "with open('/content/gdrive/My Drive/Machine-Learning-Projects/cs231n/Challenge-Data/emotiw/models/audio.train-opensmile-standardized_y.all.pickle', 'wb') as f:\n",
        "    pickle.dump(y, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AzNFJACM4E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}